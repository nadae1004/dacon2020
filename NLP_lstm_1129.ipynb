{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20201121_DACON_NLP_lstm(acc_78_18)_1129.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZdzLYTYFD2W"
      },
      "source": [
        " #경로 설정\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/dacon/20201204_NLP')"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFhXNpxSE_BD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60757d5d-859d-4b04-bb59-8be0bc209363"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMNFjVamSou7"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6uOADFRSox8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5f2870-30ea-4687-d2a2-dda4dbbe2e14"
      },
      "source": [
        "from matplotlib import rcParams, pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Conv1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import plot_model, to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import warnings \n",
        "warnings.filterwarnings(action='ignore')\n",
        "#추가\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMNDPd-MSo0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df59bf6-a4da-4c77-c9cb-7d6767d2ad50"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Restrict TensorFlow to only use the first GPU\n",
        "    try:\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "    except RuntimeError as e:\n",
        "        # Visible devices must be set before GPUs have been initialized\n",
        "        print(e)\n",
        "else:\n",
        "    print('No GPU detected')"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW92pjFeRNZ1"
      },
      "source": [
        "rcParams['figure.figsize'] = (16, 8)\n",
        "plt.style.use('fivethirtyeight')\n",
        "pd.set_option('max_columns', 100)\n",
        "pd.set_option(\"display.precision\", 4)\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yexI-zFRNcW"
      },
      "source": [
        "data_dir = Path('/content/drive/My Drive/dacon/20201204_NLP')\n",
        "feature_dir = Path('../build/feature')\n",
        "val_dir = Path('../build/val')\n",
        "tst_dir = Path('../build/tst')\n",
        "sub_dir = Path('../build/sub')\n",
        "\n",
        "dirs = [feature_dir, val_dir, tst_dir, sub_dir]\n",
        "for d in dirs:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "trn_file = data_dir / 'train.csv'\n",
        "tst_file = data_dir / 'test_x.csv'\n",
        "sample_file = data_dir / 'sample_submission.csv'\n",
        "\n",
        "target_col = 'author'\n",
        "n_fold = 5\n",
        "n_class = 5\n",
        "seed = 42"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mofw4ECEcmC"
      },
      "source": [
        "algo_name = 'lstm_1129'\n",
        "feature_name = 'emb'\n",
        "model_name = f'{algo_name}_{feature_name}'\n",
        "\n",
        "feature_file = feature_dir / f'{feature_name}.csv'\n",
        "p_val_file = val_dir / f'{model_name}.val.csv'\n",
        "p_tst_file = tst_dir / f'{model_name}.tst.csv'\n",
        "sub_file = sub_dir / f'{model_name}.csv'"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TILq23yHCsDT"
      },
      "source": [
        "trn = pd.read_csv(trn_file, index_col=0)\n",
        "tst = pd.read_csv(tst_file, index_col=0)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-l8V8qW7kLf"
      },
      "source": [
        "def data_text_cleaning(text):\n",
        " \n",
        "    # 영문자 이외 문자는 공백으로 변환\n",
        "    only_english = re.sub('[^A-Za-z0-9 ]', ' ', text)\n",
        " \n",
        "    # 소문자 변환\n",
        "    no_capitals = only_english.lower().split()\n",
        " \n",
        "    # 불용어 제거\n",
        "    stops = set(stopwords.words('english'))\n",
        "    no_stops = [word for word in no_capitals if not word in stops]\n",
        " \n",
        "    # 어간 추출\n",
        "    stemmer = nltk.stem.SnowballStemmer('english')\n",
        "    stemmer_words = [stemmer.stem(word) for word in no_stops]\n",
        " \n",
        "    # 공백으로 구분된 문자열로 결합하여 결과 반환\n",
        "    return ' '.join(stemmer_words)"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JnfIO5Y73kX"
      },
      "source": [
        "trn['text']=trn['text'].apply(data_text_cleaning)\n",
        "tst['text']=tst['text'].apply(data_text_cleaning)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChD9LYcxQt1v"
      },
      "source": [
        "clear_punc = []\n",
        "for t in range(len(trn.text)):\n",
        "  clear_punc.append(re.sub('[!@#$%^&“*()_+”=‘,./ª?\\><;\":`~]', ' ' ,trn.text[t]))\n",
        "trn['clear_punc'] = clear_punc"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Pk9yv7Q3T-"
      },
      "source": [
        "clear_punc = []\n",
        "for t in range(len(tst.text)):\n",
        "  clear_punc.append(re.sub('[!@#$%“^&*()_+=,‘./ª?\\><;\":`~]', ' ' ,tst.text[t]))\n",
        "tst['clear_punc'] = clear_punc"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huUHYQsAQ7il"
      },
      "source": [
        "trn['length_text'] = trn.clear_punc.apply(lambda x : len(x))\n",
        "tst['length_text'] = tst.clear_punc.apply(lambda x : len(x))"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpAfzM6pRGYl"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer('\\s+', gaps = True)\n",
        "token_text = []\n",
        "\n",
        "for t in trn.clear_punc:\n",
        "  temp = tokenizer.tokenize(t)\n",
        "  #temp = [word for word in temp if not word in stopwords]\n",
        "  token_text.append(temp)\n",
        "\n",
        "trn['token_text'] = token_text"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP6TsBQORd5U"
      },
      "source": [
        "token_text = []\n",
        "\n",
        "for t in tst.clear_punc:\n",
        "  temp = tokenizer.tokenize(t)\n",
        "  #temp = [word for word in temp if not word in stopwords]\n",
        "  token_text.append(temp)\n",
        "\n",
        "tst['token_text'] = token_text"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2knbAMWdRhbD"
      },
      "source": [
        "trn['length_token'] = trn.token_text.apply(lambda x : len(x))\n",
        "tst['length_token'] = tst.token_text.apply(lambda x : len(x))"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO0n7_qqRki7"
      },
      "source": [
        "trn = trn.loc[trn.length_token > 6]"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXk7GTo2RoAL"
      },
      "source": [
        "trn.index = range(trn.shape[0])"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        },
        "id": "zlhLSLpiRqok",
        "outputId": "6ec74c9f-636a-49fd-a236-1889c0ca8a8b"
      },
      "source": [
        "trn"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>clear_punc</th>\n",
              "      <th>length_text</th>\n",
              "      <th>token_text</th>\n",
              "      <th>length_token</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>almost choke much much want say strang exclam ...</td>\n",
              "      <td>3</td>\n",
              "      <td>almost choke much much want say strang exclam ...</td>\n",
              "      <td>107</td>\n",
              "      <td>[almost, choke, much, much, want, say, strang,...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>engag one day walk perus jane last letter dwel...</td>\n",
              "      <td>1</td>\n",
              "      <td>engag one day walk perus jane last letter dwel...</td>\n",
              "      <td>162</td>\n",
              "      <td>[engag, one, day, walk, perus, jane, last, let...</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>captain porch keep care way treacher shot inte...</td>\n",
              "      <td>4</td>\n",
              "      <td>captain porch keep care way treacher shot inte...</td>\n",
              "      <td>169</td>\n",
              "      <td>[captain, porch, keep, care, way, treacher, sh...</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>merci gentlemen odin flung hand write anyway s...</td>\n",
              "      <td>3</td>\n",
              "      <td>merci gentlemen odin flung hand write anyway s...</td>\n",
              "      <td>106</td>\n",
              "      <td>[merci, gentlemen, odin, flung, hand, write, a...</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>well fought said sooth charg us twice</td>\n",
              "      <td>4</td>\n",
              "      <td>well fought said sooth charg us twice</td>\n",
              "      <td>37</td>\n",
              "      <td>[well, fought, said, sooth, charg, us, twice]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54871</th>\n",
              "      <td>sir odin great caus thank said princ</td>\n",
              "      <td>4</td>\n",
              "      <td>sir odin great caus thank said princ</td>\n",
              "      <td>36</td>\n",
              "      <td>[sir, odin, great, caus, thank, said, princ]</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54873</th>\n",
              "      <td>look back shoulder beckon enter mr odin got ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>look back shoulder beckon enter mr odin got ar...</td>\n",
              "      <td>89</td>\n",
              "      <td>[look, back, shoulder, beckon, enter, mr, odin...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54874</th>\n",
              "      <td>mr smith odin whisper hard dare hope would come</td>\n",
              "      <td>2</td>\n",
              "      <td>mr smith odin whisper hard dare hope would come</td>\n",
              "      <td>47</td>\n",
              "      <td>[mr, smith, odin, whisper, hard, dare, hope, w...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54875</th>\n",
              "      <td>told plan captain us settl detail accomplish</td>\n",
              "      <td>4</td>\n",
              "      <td>told plan captain us settl detail accomplish</td>\n",
              "      <td>44</td>\n",
              "      <td>[told, plan, captain, us, settl, detail, accom...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54876</th>\n",
              "      <td>sincer well wisher friend sister luci odin</td>\n",
              "      <td>1</td>\n",
              "      <td>sincer well wisher friend sister luci odin</td>\n",
              "      <td>42</td>\n",
              "      <td>[sincer, well, wisher, friend, sister, luci, o...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37927 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    text  author  \\\n",
              "index                                                              \n",
              "0      almost choke much much want say strang exclam ...       3   \n",
              "2      engag one day walk perus jane last letter dwel...       1   \n",
              "3      captain porch keep care way treacher shot inte...       4   \n",
              "4      merci gentlemen odin flung hand write anyway s...       3   \n",
              "5                  well fought said sooth charg us twice       4   \n",
              "...                                                  ...     ...   \n",
              "54871               sir odin great caus thank said princ       4   \n",
              "54873  look back shoulder beckon enter mr odin got ar...       0   \n",
              "54874    mr smith odin whisper hard dare hope would come       2   \n",
              "54875       told plan captain us settl detail accomplish       4   \n",
              "54876         sincer well wisher friend sister luci odin       1   \n",
              "\n",
              "                                              clear_punc  length_text  \\\n",
              "index                                                                   \n",
              "0      almost choke much much want say strang exclam ...          107   \n",
              "2      engag one day walk perus jane last letter dwel...          162   \n",
              "3      captain porch keep care way treacher shot inte...          169   \n",
              "4      merci gentlemen odin flung hand write anyway s...          106   \n",
              "5                  well fought said sooth charg us twice           37   \n",
              "...                                                  ...          ...   \n",
              "54871               sir odin great caus thank said princ           36   \n",
              "54873  look back shoulder beckon enter mr odin got ar...           89   \n",
              "54874    mr smith odin whisper hard dare hope would come           47   \n",
              "54875       told plan captain us settl detail accomplish           44   \n",
              "54876         sincer well wisher friend sister luci odin           42   \n",
              "\n",
              "                                              token_text  length_token  \n",
              "index                                                                   \n",
              "0      [almost, choke, much, much, want, say, strang,...            20  \n",
              "2      [engag, one, day, walk, perus, jane, last, let...            29  \n",
              "3      [captain, porch, keep, care, way, treacher, sh...            31  \n",
              "4      [merci, gentlemen, odin, flung, hand, write, a...            18  \n",
              "5          [well, fought, said, sooth, charg, us, twice]             7  \n",
              "...                                                  ...           ...  \n",
              "54871       [sir, odin, great, caus, thank, said, princ]             7  \n",
              "54873  [look, back, shoulder, beckon, enter, mr, odin...            16  \n",
              "54874  [mr, smith, odin, whisper, hard, dare, hope, w...             9  \n",
              "54875  [told, plan, captain, us, settl, detail, accom...             7  \n",
              "54876  [sincer, well, wisher, friend, sister, luci, o...             7  \n",
              "\n",
              "[37927 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flRnlfjjRykH"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Ni6j02R0jr"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(trn.token_text)"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEyDotuJR4KN",
        "outputId": "1956e895-250b-4843-eb69-ca0d889749ed"
      },
      "source": [
        "threshold = 6\n",
        "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
        "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
        "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
        "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
        "\n",
        "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
        "for key, value in tokenizer.word_counts.items():\n",
        "    total_freq = total_freq + value\n",
        "\n",
        "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
        "    if(value < threshold):\n",
        "        rare_cnt = rare_cnt + 1\n",
        "        rare_freq = rare_freq + value\n",
        "\n",
        "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
        "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
        "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
        "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합(vocabulary)의 크기 : 19576\n",
            "등장 빈도가 5번 이하인 희귀 단어의 수: 11073\n",
            "단어 집합에서 희귀 단어의 비율: 56.56416019615856\n",
            "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 2.20160674252675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBXgcfYXSA-w",
        "outputId": "57dbbcca-9bce-4d6d-e4df-32bbbdcb5a33"
      },
      "source": [
        "vocab_size = total_cnt - rare_cnt + 1 # 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거. 0번 패딩 토큰을 고려하여 +1\n",
        "print('단어 집합의 크기 :',vocab_size)"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 8504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff--pphMSEZT"
      },
      "source": [
        "tokenizer = Tokenizer(vocab_size,oov_token = 'oov')\n",
        "tokenizer.fit_on_texts(trn.token_text)"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JJtFmgOR5VF"
      },
      "source": [
        "def below_threshold_len(max_len, nested_list):\n",
        "  cnt = 0\n",
        "  for s in nested_list:\n",
        "    if(len(s) <= max_len):\n",
        "        cnt = cnt + 1\n",
        "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh0vJ9RHR5SZ",
        "outputId": "feea04b7-0329-4654-b8b3-b1057a81535f"
      },
      "source": [
        "below_threshold_len(300, trn.token_text)"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "전체 샘플 중 길이가 300 이하인 샘플의 비율: 100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Wc5ZxOR5Pq"
      },
      "source": [
        "y = np.array([x for x in trn['author']])"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN_TbOCGR5KM"
      },
      "source": [
        "max_len =  300"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbXTuGJTR5GT"
      },
      "source": [
        "trn = tokenizer.texts_to_sequences(trn.token_text)\n",
        "trn = pad_sequences(seq, maxlen = max_len, padding = 'post')"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhQRp1WOSrBS"
      },
      "source": [
        "tst = tokenizer.texts_to_sequences(tst.token_text)\n",
        "tst = pad_sequences(target, maxlen = max_len, padding = 'post')"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSL_qm-xVE7l"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "glove = dict()\n",
        "f = open('/content/drive/MyDrive/dacon/20201204_NLP/glove.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vector = np.asarray(values[1:], dtype='float32')\n",
        "    glove[word] = vector\n",
        "f.close()"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Y2BI1FV628"
      },
      "source": [
        "embedding_matrix = np.zeros((vocab_size, 100)) #100차원의 임베딩 매트릭스 생성\n",
        "\n",
        "for index, word in enumerate(word): #vocabulary에 있는 토큰들을 하나씩 넘겨줍니다.\n",
        "    if word in glove: #넘겨 받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 뜻)\n",
        "        embedding_vector = glove[word] #해당 토큰에 해당하는 vector를 불러오고\n",
        "        embedding_matrix[i] = embedding_vector #해당 위치의 embedding_mxtrix에 저장합니다.\n",
        "    else:\n",
        "        print(\"glove 없는 단어입니다.\")\n",
        "        break"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xIAUzRUsHTF"
      },
      "source": [
        "**lstm 모델**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WFtcxjvMk0H"
      },
      "source": [
        "embedding_dim = 64"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKHZ6dKDpsUn"
      },
      "source": [
        "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmNwwGCTpsfq"
      },
      "source": [
        "def get_model():\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, weights = [embedding_matrix], input_length=max_len),\n",
        "        Dropout(.2),\n",
        "        Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        Bidirectional(LSTM(64)),\n",
        "        Dense(n_class, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=.01))\n",
        "    return model"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jK-vAXOptLW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "7d54dd45-3e8d-4e85-c7f6-596e6b808255"
      },
      "source": [
        "p_val = np.zeros((trn.shape[0], n_class))\n",
        "p_tst = np.zeros((tst.shape[0], n_class))\n",
        "for i, (i_trn, i_val) in enumerate(cv.split(trn, y), 1):\n",
        "    print(f'training model for CV #{i}')\n",
        "    clf = get_model()\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3,\n",
        "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
        "\n",
        "    clf.fit(trn[i_trn], \n",
        "            to_categorical(y[i_trn]),\n",
        "            validation_data=(trn[i_val], to_categorical(y[i_val])),\n",
        "            epochs=10,\n",
        "            batch_size=512,\n",
        "            callbacks=[es])\n",
        "    p_val[i_val, :] = clf.predict(trn[i_val])\n",
        "    p_tst += clf.predict(tst) / n_fold"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-214-bb6fb7cd7f8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp_tst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'training model for CV #{i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0mset\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \"\"\"\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [37916, 37927]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0LYVZpsptXm"
      },
      "source": [
        "print(f'Accuracy (CV): {accuracy_score(y, np.argmax(p_val, axis=1)) * 100:8.4f}%')\n",
        "print(f'Log Loss (CV): {log_loss(pd.get_dummies(y), p_val):8.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YhBE-FPptkQ"
      },
      "source": [
        "np.savetxt(p_val_file, p_val, fmt='%.6f', delimiter=',')\n",
        "np.savetxt(p_tst_file, p_tst, fmt='%.6f', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKX68uIyptwX"
      },
      "source": [
        "print(clf.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbSaWPyi62Oa"
      },
      "source": [
        "plot_model(clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tacCUhxmpuHh"
      },
      "source": [
        "sub = pd.read_csv(sample_file, index_col=0)\n",
        "print(sub.shape)\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJfaRpGxDU1m"
      },
      "source": [
        "sub[sub.columns] = p_tst\n",
        "sub.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGXIqy7jDVR1"
      },
      "source": [
        "sub.to_csv(sub_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}